---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

* **Graduate Student Researcher**, Digital Circuits and Systems Lab, NTU, Taipei, Taiwan, 07/2019 - now
  * <u>Advisor: Prof. Chia-Hsiang Yang</u>
  * Designed an energy-efficient processor for sparse NN training (in collaboration with Qualcomm)
    * Carried out complete ASIC design flow, including algorithm design, chip tape-out, and chip measurement
    * The proposed design outperforms the state-of-the-art work by 3.7$\times$ improvement in energy-efficiency
  * Evaluated the energy and area of 3D circuits for AI applications (in collaboration with TSMC)
  * Developed an accelerator for DNN inference (in collaboration with Digwise)
  * Participated in the image deblurring IC project
  * Developed an in-house tool to make SoC designs utilizing picorv/Cortex-M3 easier
    * The tool supports generation of bus RTL files and header files from a system-describing JSON file
    * The tool chains C code compilation to RTL simulation of userâ€™s testcases
* **Intern**, MemryX Incorporation, Taipei, Taiwan, 12/2019 - 08/2021
  * Developed a memory access optimization methodology for CNN computation
  * Designed peripheral controllers (including efuse, QSPI flash) for SoC integration of the new version product
* **Undergraduate Student Researcher**, Digital Circuits and Systems Lab, NTU, Taipei, Taiwan, 08/2018 - 06/2019
  * <u>Advisor: Prof. Chia-Hsiang Yang</u>
  * Conducted an in-house toolchain for mapping deep learning models to a designed processor
  * Designed a dedicated processor (RTL to P&R), a model mapping algorithm, and an user interface
* **Intern**, Genesys Logic Incorporation, Taipei, Taiwan, 02/2019 - 06/2019
  * Developed a compact NN model for multiple object detection on an in-house dataset
* **Research Assistant**, Institute of Information Science, Academia Sinica, Taipei, Taiwan, 03/2018 - 06/2019
  * <u>Advisor: Dr. Li Su</u>
  * Developed a neural network for automatic music segmentation and transcription
  * The proposed model outperforms all previous methods in transcription by 7.4% in terms of the F1-measure

<!--
{% include base_path %}

{% for post in site.teaching reversed %}
  {% include archive-single.html %}
{% endfor %}
-->